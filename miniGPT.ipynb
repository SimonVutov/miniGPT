{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch tqdm transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\simeo\\OneDrive\\Documents\\GitHub\\miniGPT\\torch-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset...\n",
      "Initializing the GPT-2 tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\simeo\\OneDrive\\Documents\\GitHub\\miniGPT\\torch-env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 20233texts [00:47, 261.38texts/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved token_batches\\tokens_batch_0.pt with 10000009 tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 40627texts [01:35, 265.32texts/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved token_batches\\tokens_batch_1.pt with 10000136 tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 60943texts [02:20, 238.05texts/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved token_batches\\tokens_batch_2.pt with 10000234 tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 81357texts [03:06, 250.48texts/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved token_batches\\tokens_batch_3.pt with 10000322 tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 101594texts [03:48, 214.89texts/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved token_batches\\tokens_batch_4.pt with 10000625 tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 122063texts [04:32, 307.59texts/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved token_batches\\tokens_batch_5.pt with 10000059 tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 142262texts [05:22, 233.37texts/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved token_batches\\tokens_batch_6.pt with 10000436 tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 162672texts [06:09, 226.07texts/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved token_batches\\tokens_batch_7.pt with 10000026 tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 182942texts [06:51, 257.44texts/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved token_batches\\tokens_batch_8.pt with 10000012 tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 203273texts [07:34, 244.35texts/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved token_batches\\tokens_batch_9.pt with 10000071 tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 223629texts [08:18, 259.10texts/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved token_batches\\tokens_batch_10.pt with 10000451 tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 244075texts [09:02, 277.96texts/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved token_batches\\tokens_batch_11.pt with 10000641 tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 264214texts [09:46, 177.02texts/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved token_batches\\tokens_batch_12.pt with 10000681 tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 272498texts [10:02, 507.32texts/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2TokenizerFast\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_dir = 'token_batches'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load the dataset from Hugging Face\n",
    "print(\"Loading the dataset...\")\n",
    "dataset = load_dataset(\"HuggingFaceFW/fineweb\", \"sample-10BT\", split='train', streaming=True)\n",
    "\n",
    "# Initialize the GPT-2 tokenizer\n",
    "print(\"Initializing the GPT-2 tokenizer...\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Tokenization process\n",
    "print(\"Starting tokenization...\")\n",
    "tokens_per_file = 10000000  # Save after processing 10 million tokens\n",
    "all_tokens = []\n",
    "token_count = 0\n",
    "file_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    for sample in tqdm(dataset, desc=\"Processing texts\", unit=\"texts\"):\n",
    "        text = sample['text']\n",
    "        newly_encoded_tokens = tokenizer.encode(text, truncation=True, padding=False)\n",
    "        all_tokens.extend(newly_encoded_tokens)\n",
    "        token_count += len(newly_encoded_tokens)\n",
    "        \n",
    "        if token_count >= tokens_per_file:\n",
    "            file_path = os.path.join(output_dir, f'tokens_batch_{file_count}.pt')\n",
    "            torch.save(torch.tensor(all_tokens, dtype=torch.long), file_path)\n",
    "            print(f\"Saved {file_path} with {len(all_tokens)} tokens.\")\n",
    "            file_count += 1\n",
    "            all_tokens = []  # Clear the list to free memory\n",
    "            token_count = 0\n",
    "\n",
    "    # Save any remaining tokens after the loop\n",
    "    if all_tokens:\n",
    "        file_path = os.path.join(output_dir, f'tokens_batch_{file_count}.pt')\n",
    "        torch.save(torch.tensor(all_tokens, dtype=torch.long), file_path)\n",
    "        print(f\"Saved {file_path} with {len(all_tokens)} tokens.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Tokenization completed in {elapsed_time:.2f} seconds.\")\n",
    "print(\"All tokens saved in 'token_batches' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int\n",
    "    vocab_size: int\n",
    "    n_layer: int\n",
    "    n_head: int\n",
    "    n_embd: int\n",
    "\n",
    "class TokenizedDataset(IterableDataset):\n",
    "    def __init__(self, token_files, block_size, batch_size):\n",
    "        super().__init__()\n",
    "        self.token_files = token_files\n",
    "        self.block_size = block_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def read_tokens(self, file_path):\n",
    "        tokens = torch.load(file_path)\n",
    "        for i in range(0, len(tokens), self.block_size + 1):\n",
    "            yield tokens[i:i+self.block_size+1]\n",
    "\n",
    "    def __iter__(self):\n",
    "        for file_path in self.token_files:\n",
    "            tokens = torch.load(file_path)\n",
    "            total_tokens = len(tokens)\n",
    "            total_batches = (total_tokens // (self.block_size + 1)) // self.batch_size\n",
    "            print(\"Path: \", file_path, \"Total tokens in file:\", total_tokens, \"Total batches in file will be:\", total_batches)\n",
    "            token_generator = self.read_tokens(file_path)\n",
    "            for tokens in token_generator:\n",
    "                if len(tokens) == self.block_size + 1:\n",
    "                    input_ids = tokens[:-1].clone().detach().to(dtype=torch.long)\n",
    "                    labels = tokens[1:].clone().detach().to(dtype=torch.long)\n",
    "                    yield input_ids, labels\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=config.n_embd, num_heads=config.n_head)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x), self.ln1(x), self.ln1(x))[0]\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.position_embedding = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(config) for _ in range(config.n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
    "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n",
    "        x = self.token_embedding(x) + self.position_embedding[:, :x.size(1), :]\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch transformers tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2TokenizerFast\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from model import GPT, GPTConfig, TokenizedDataset\n",
    "from torch import nn\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "BATCH_SIZE = 32  # 16\n",
    "ACCUMULATION_STEPS = 4  # 4 Accumulate gradients over this many steps\n",
    "PRINT_EVERY = 200  # Print training loss every this many batches\n",
    "\n",
    "# Set num_workers to the number of logical processors\n",
    "num_workers = os.cpu_count()\n",
    "\n",
    "def save_model_and_optimizer(model, optimizer, epoch, file_idx):\n",
    "    os.makedirs('checkpoints', exist_ok=True)\n",
    "    checkpoint_path = f'checkpoints/gpt2_epoch_{epoch + 1}_file_{file_idx + 1}.pt'\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'file_idx': file_idx\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "def load_model_and_optimizer(model, optimizer):\n",
    "    start_epoch = 0\n",
    "    if os.path.exists('checkpoints'):\n",
    "        checkpoint_files = [f for f in os.listdir('checkpoints') if f.startswith('gpt2_epoch_')]\n",
    "        if checkpoint_files:\n",
    "            latest_checkpoint = max(checkpoint_files, key=lambda x: (int(x.split('_')[2]), int(x.split('_')[4].split('.')[0])))\n",
    "            checkpoint = torch.load(f'checkpoints/{latest_checkpoint}')\n",
    "            if 'model_state_dict' in checkpoint and 'optimizer_state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                start_epoch = checkpoint['epoch'] + 1\n",
    "                print(f\"Model and optimizer loaded from checkpoint '{latest_checkpoint}'\")\n",
    "                return start_epoch\n",
    "            else:\n",
    "                print(\"Checkpoint file is missing required keys\")\n",
    "    return start_epoch\n",
    "\n",
    "def generate_text(inputText):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    input_ids = tokenizer.encode(inputText, return_tensors=\"pt\").to(device)\n",
    "    for _ in range(40):\n",
    "        with autocast():  # Mixed precision inference\n",
    "            logits = model(input_ids)\n",
    "        next_token = torch.multinomial(nn.functional.softmax(logits[:, -1, :], dim=-1), num_samples=1)\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "    generated_text = tokenizer.decode(input_ids[0].tolist())\n",
    "    print(f\"Generated text: {generated_text}\")\n",
    "\n",
    "def train(model, loader, optimizer):\n",
    "    start_epoch = load_model_and_optimizer(model, optimizer)\n",
    "    print(f\"Starting training from epoch {start_epoch + 1}\")\n",
    "    scaler = GradScaler()\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    last_print_time = time.time()  # Initialize last print time\n",
    "\n",
    "    # Open the loss file in append mode\n",
    "    with open('loss.txt', 'a') as loss_file:\n",
    "        for epoch in range(start_epoch, start_epoch + 5):\n",
    "            for batch_idx, (input_ids, labels) in enumerate(loader, start=1):\n",
    "                input_ids, labels = input_ids.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with autocast():\n",
    "                    logits = model(input_ids)\n",
    "                    loss = nn.CrossEntropyLoss()(logits.view(-1, config.vocab_size), labels.view(-1))\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                if (batch_idx + 1) % ACCUMULATION_STEPS == 0:\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                tokens_per_batch = input_ids.numel()  # Total number of tokens in the batch\n",
    "                current_time = time.time()\n",
    "                elapsed_time = current_time - last_print_time  # Time since last print\n",
    "                tokens_per_second = PRINT_EVERY * tokens_per_batch / elapsed_time if elapsed_time > 0 else 0\n",
    "\n",
    "                if batch_idx % PRINT_EVERY == 0:\n",
    "                    time_since_start = current_time - start_time\n",
    "                    print(f\"Epoch {epoch + 1}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "                    print(f\"Tokens in batch: {tokens_per_batch}, Elapsed time: {elapsed_time:.4f} sec, Tokens/sec: {tokens_per_second:.2f}\")\n",
    "                    print(f\"Time Elapsed since start: {time_since_start:.2f} sec\")\n",
    "                    last_print_time = current_time  # Update last print time\n",
    "\n",
    "                    # Save the loss to the file\n",
    "                    loss_file.write(f\"Epoch {epoch + 1}, Batch {batch_idx}, Loss: {loss.item():.4f}\\n\")\n",
    "\n",
    "                if batch_idx % 1000 == 0:\n",
    "                    save_model_and_optimizer(model, optimizer, epoch, 0)\n",
    "                    generate_text(\"I am a\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the global counter in the main script and pass it to the dataset class\n",
    "    counter0 = 0\n",
    "\n",
    "    # Initialization and data loading\n",
    "    token_files = [os.path.join('token_batches', f) for f in os.listdir('token_batches') if f.startswith('tokens_batch_')]\n",
    "    config = GPTConfig(block_size=128, vocab_size=50257, n_layer=12, n_head=12, n_embd=768)\n",
    "    dataset = TokenizedDataset(token_files, config.block_size, batch_size=BATCH_SIZE)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    # Setup model and optimizer\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"Device: \", device)\n",
    "    model = GPT(config).to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=0.0003)\n",
    "\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "    train(model, loader, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
